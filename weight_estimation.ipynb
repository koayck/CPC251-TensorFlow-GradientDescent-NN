{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b9b28a8",
   "metadata": {},
   "source": [
    "#### Group Information\n",
    "\n",
    "Group No: \n",
    "\n",
    "- Member 1: Lai Yicheng\n",
    "- Member 2: Lee Ying Shen\n",
    "- Member 3: Lim Ting Juin\n",
    "- Member 4: Koay Chun Keat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0a3c45",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79b84136",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi=False\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set a random seed for reproducible results \n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dceb4ff",
   "metadata": {},
   "source": [
    "#### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a83d38c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.422609</td>\n",
       "      <td>0.608152</td>\n",
       "      <td>5.572301</td>\n",
       "      <td>0.665370</td>\n",
       "      <td>5.495197</td>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.105038</td>\n",
       "      <td>6.983217</td>\n",
       "      <td>5.840074</td>\n",
       "      <td>6.439401</td>\n",
       "      <td>4.186770</td>\n",
       "      <td>0.50025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-16.877003</td>\n",
       "      <td>-18.725112</td>\n",
       "      <td>-16.255804</td>\n",
       "      <td>-13.320196</td>\n",
       "      <td>-5.700803</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-3.073090</td>\n",
       "      <td>-4.289903</td>\n",
       "      <td>1.759988</td>\n",
       "      <td>-4.489635</td>\n",
       "      <td>2.668686</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.447864</td>\n",
       "      <td>0.967124</td>\n",
       "      <td>5.760596</td>\n",
       "      <td>-0.465704</td>\n",
       "      <td>5.558958</td>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.743536</td>\n",
       "      <td>5.657798</td>\n",
       "      <td>9.556160</td>\n",
       "      <td>5.528407</td>\n",
       "      <td>8.388997</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.376673</td>\n",
       "      <td>17.904490</td>\n",
       "      <td>24.342184</td>\n",
       "      <td>20.199927</td>\n",
       "      <td>20.973491</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                f1           f2           f3           f4           f5   \n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  \\\n",
       "mean      0.422609     0.608152     5.572301     0.665370     5.495197   \n",
       "std       5.105038     6.983217     5.840074     6.439401     4.186770   \n",
       "min     -16.877003   -18.725112   -16.255804   -13.320196    -5.700803   \n",
       "25%      -3.073090    -4.289903     1.759988    -4.489635     2.668686   \n",
       "50%       0.447864     0.967124     5.760596    -0.465704     5.558958   \n",
       "75%       3.743536     5.657798     9.556160     5.528407     8.388997   \n",
       "max      15.376673    17.904490    24.342184    20.199927    20.973491   \n",
       "\n",
       "            label  \n",
       "count  1000.00000  \n",
       "mean      0.50000  \n",
       "std       0.50025  \n",
       "min       0.00000  \n",
       "25%       0.00000  \n",
       "50%       0.50000  \n",
       "75%       1.00000  \n",
       "max       1.00000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('classification_dataset.csv')\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0f56c6",
   "metadata": {},
   "source": [
    "#### Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4024775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    This function calculates the loss function\n",
    "    \"\"\"\n",
    "\n",
    "    # Setting a minimum and maximum value to prevent log(0) and log(1)\n",
    "    # epsilon = 1e-7\n",
    "    # y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "\n",
    "    # Binary Negative Log-Likelihood \n",
    "    epsilon = 1e-7\n",
    "    return -tf.reduce_mean(y_true * tf.math.log(y_pred + epsilon) + (1 - y_true) * tf.math.log(1 - y_pred + epsilon))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753a2eca",
   "metadata": {},
   "source": [
    "#### Define function to perform prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdbf2168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(output):\n",
    "    \"\"\" \n",
    "    This function calculates the sigmoid function.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + tf.exp(-output))\n",
    "\n",
    "def relu(inputs):\n",
    "    \"\"\" \n",
    "    This function calculates the ReLU function.\n",
    "    \"\"\"\n",
    "    return np.maximum(0.0, inputs)\n",
    "\n",
    "def forward(inputs, weights, biases):\n",
    "    \"\"\" \n",
    "    This function calculates the forward pass (predicts the label).\n",
    "    \"\"\"\n",
    "    activation = 0\n",
    "    num_layers = len(weights) - 1\n",
    "    for i in range(num_layers):\n",
    "        weighted_sum = tf.matmul(inputs, weights[i], False, False) + biases[i]\n",
    "        activation = relu(weighted_sum)\n",
    "\n",
    "    weighted_sum_output = tf.matmul(activation, weights[-1], False, False) + biases[-1]\n",
    "    prediction = sigmoid(weighted_sum_output)\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bc735b",
   "metadata": {},
   "source": [
    "#### Define function for model training\n",
    "Display the training and validation loss values for each epoch of the training loop. The displayed value must be in 6 decimal places.<br>\n",
    "Hint: <br>\n",
    "Use `tf.GradientTape` to compute the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe17ccc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(inputs, targets, weights, biases, learning_rate):\n",
    "    \"\"\" \n",
    "    This function performs the forward pass, computes the gradient and update the weights and biases.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        outputs = forward(inputs, weights, biases)\n",
    "        loss = loss_fn(targets, outputs)\n",
    "\n",
    "    for i in range(len(weights)):\n",
    "        weights_grad = tape.gradient(loss, weights[i])\n",
    "        biases_grad = tape.gradient(loss, biases[i])\n",
    "        # Update weights and biases\n",
    "        print(weights_grad)\n",
    "        weights[i].assign_sub(learning_rate * weights_grad)\n",
    "        biases[i].assign_sub(learning_rate * biases_grad)\n",
    "\n",
    "    # Delete the tape after using it\n",
    "    del tape\n",
    "\n",
    "    return weights, biases, loss\n",
    "\n",
    "def fit(train_dataset, valid_dataset, weights, biases, learning_rate, epochs):\n",
    "    \"\"\" \n",
    "    This function implements the training loop.\n",
    "    \"\"\"\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training loop\n",
    "        for inputs_batch, targets_batch in train_dataset:\n",
    "            weights, biases, loss = train(inputs_batch, targets_batch, weights, biases, learning_rate)\n",
    "        train_losses.append(loss)\n",
    "\n",
    "        # Validation loop\n",
    "        valid_loss_avg = 0\n",
    "        valid_steps = 0\n",
    "        for valid_inputs, valid_targets in valid_dataset:\n",
    "            valid_outputs = forward(valid_inputs, weights, biases)\n",
    "            valid_loss = loss_fn(valid_targets, valid_outputs)\n",
    "            valid_loss_avg += valid_loss\n",
    "            valid_steps += 1\n",
    "        avg_valid_loss = valid_loss_avg / valid_steps\n",
    "        valid_losses.append(avg_valid_loss)\n",
    "\n",
    "        print(f\"Epoch: {epoch}, Train Loss: {loss}, Validation Loss: {avg_valid_loss}\")\n",
    "\n",
    "    return weights, biases, train_losses, valid_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f45213",
   "metadata": {},
   "source": [
    "#### Define the tensors to hold the weights and biases (create the model)\n",
    "Hint: <br>\n",
    "Use `tf.Variable` to create the tensors.<br>\n",
    "Put the tensors in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2e2172b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = dataset.iloc[:, :-1]  # All rows, all columns except the last\n",
    "labels = dataset.iloc[:, -1]  # All rows, only the last column\n",
    "\n",
    "# Convert the pandas DataFrame into a TensorFlow Dataset\n",
    "dataset_tf = tf.data.Dataset.from_tensor_slices((features.values, labels.values))\n",
    "\n",
    "input_size = 5  # 5 features\n",
    "hidden_size = 8  # Size of the hidden layer\n",
    "output_size = 1  # Predict 2 classes\n",
    "\n",
    "# Initialize weights and biases for each layer in lists\n",
    "weights = [tf.Variable(tf.random.normal([input_size, hidden_size], stddev=0.1)),\n",
    "           tf.Variable(tf.random.normal([hidden_size, output_size], stddev=0.1))]\n",
    "\n",
    "biases = [tf.Variable(tf.zeros([hidden_size])),\n",
    "          tf.Variable(tf.zeros([output_size]))]\n",
    "\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176badb8",
   "metadata": {},
   "source": [
    "#### Split the dataset\n",
    "The ratio of training and test is 7:1:2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fa1b9b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TakeDataset element_spec=(TensorSpec(shape=(5,), dtype=tf.float64, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle the dataset\n",
    "dataset_tf = dataset_tf.shuffle(buffer_size=len(dataset_tf), seed=42)\n",
    "\n",
    "# Calculate the number of examples\n",
    "total_size = len(dataset_tf)\n",
    "train_size = int(0.7 * total_size)\n",
    "valid_size = int(0.1 * total_size)\n",
    "# The rest is used for testing\n",
    "\n",
    "# Create the training, validation and test sets\n",
    "train_dataset = dataset_tf.take(train_size)\n",
    "valid_dataset = dataset_tf.skip(train_size).take(valid_size)\n",
    "test_dataset = dataset_tf.skip(train_size + valid_size)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c4d6cf",
   "metadata": {},
   "source": [
    "#### Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f689b7c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=(TensorSpec(shape=(32, 5), dtype=tf.float32, name=None), TensorSpec(shape=(32,), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to normalize the data\n",
    "def normalize_data(features, labels):\n",
    "    features = tf.cast(features, tf.float32)\n",
    "    features = (features - tf.reduce_min(features)) / (tf.reduce_max(features) - tf.reduce_min(features))\n",
    "    return features, labels\n",
    "\n",
    "# Apply the normalization function to the datasets\n",
    "train_dataset = train_dataset.map(normalize_data)\n",
    "valid_dataset = valid_dataset.map(normalize_data)\n",
    "test_dataset = test_dataset.map(normalize_data)\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.batch(batch_size=32, drop_remainder=True)\n",
    "valid_dataset = valid_dataset.batch(batch_size=32, drop_remainder=True)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a2e7d6",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6304c496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m weights, biases, train_losses, valid_losses \u001b[38;5;241m=\u001b[39m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbiases\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 33\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(train_dataset, valid_dataset, weights, biases, learning_rate, epochs)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m inputs_batch, targets_batch \u001b[38;5;129;01min\u001b[39;00m train_dataset:\n\u001b[1;32m---> 33\u001b[0m         weights, biases, loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbiases\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# Validation loop\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[16], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(inputs, targets, weights, biases, learning_rate)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Update weights and biases\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(weights_grad)\n\u001b[1;32m---> 14\u001b[0m     weights[i]\u001b[38;5;241m.\u001b[39massign_sub(\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mweights_grad\u001b[49m)\n\u001b[0;32m     15\u001b[0m     biases[i]\u001b[38;5;241m.\u001b[39massign_sub(learning_rate \u001b[38;5;241m*\u001b[39m biases_grad)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Delete the tape after using it\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "weights, biases, train_losses, valid_losses = fit(train_dataset, valid_dataset, weights, biases, learning_rate, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c41885",
   "metadata": {},
   "source": [
    "#### Display the training loss and validation loss against epoch graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f05472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c0b4c38",
   "metadata": {},
   "source": [
    "#### Predict the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84f73b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20d715ef",
   "metadata": {},
   "source": [
    "#### Display the confusion matrix and the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35deeb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.1 0.5 1.  1. ], shape=(4,), dtype=float32)\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
