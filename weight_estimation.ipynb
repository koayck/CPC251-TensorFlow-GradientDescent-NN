{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b9b28a8",
   "metadata": {},
   "source": [
    "#### Group Information\n",
    "\n",
    "Group No: \n",
    "\n",
    "- Member 1: Lai Yicheng\n",
    "- Member 2: Lee Ying Shen\n",
    "- Member 3: Lim Ting Juin\n",
    "- Member 4: Koay Chun Keat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0a3c45",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "79b84136",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi=False\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set a random seed for reproducible results \n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dceb4ff",
   "metadata": {},
   "source": [
    "#### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a83d38c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.422609</td>\n",
       "      <td>0.608152</td>\n",
       "      <td>5.572301</td>\n",
       "      <td>0.665370</td>\n",
       "      <td>5.495197</td>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.105038</td>\n",
       "      <td>6.983217</td>\n",
       "      <td>5.840074</td>\n",
       "      <td>6.439401</td>\n",
       "      <td>4.186770</td>\n",
       "      <td>0.50025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-16.877003</td>\n",
       "      <td>-18.725112</td>\n",
       "      <td>-16.255804</td>\n",
       "      <td>-13.320196</td>\n",
       "      <td>-5.700803</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-3.073090</td>\n",
       "      <td>-4.289903</td>\n",
       "      <td>1.759988</td>\n",
       "      <td>-4.489635</td>\n",
       "      <td>2.668686</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.447864</td>\n",
       "      <td>0.967124</td>\n",
       "      <td>5.760596</td>\n",
       "      <td>-0.465704</td>\n",
       "      <td>5.558958</td>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.743536</td>\n",
       "      <td>5.657798</td>\n",
       "      <td>9.556160</td>\n",
       "      <td>5.528407</td>\n",
       "      <td>8.388997</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.376673</td>\n",
       "      <td>17.904490</td>\n",
       "      <td>24.342184</td>\n",
       "      <td>20.199927</td>\n",
       "      <td>20.973491</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                f1           f2           f3           f4           f5   \n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  \\\n",
       "mean      0.422609     0.608152     5.572301     0.665370     5.495197   \n",
       "std       5.105038     6.983217     5.840074     6.439401     4.186770   \n",
       "min     -16.877003   -18.725112   -16.255804   -13.320196    -5.700803   \n",
       "25%      -3.073090    -4.289903     1.759988    -4.489635     2.668686   \n",
       "50%       0.447864     0.967124     5.760596    -0.465704     5.558958   \n",
       "75%       3.743536     5.657798     9.556160     5.528407     8.388997   \n",
       "max      15.376673    17.904490    24.342184    20.199927    20.973491   \n",
       "\n",
       "            label  \n",
       "count  1000.00000  \n",
       "mean      0.50000  \n",
       "std       0.50025  \n",
       "min       0.00000  \n",
       "25%       0.00000  \n",
       "50%       0.50000  \n",
       "75%       1.00000  \n",
       "max       1.00000  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('classification_dataset.csv')\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0f56c6",
   "metadata": {},
   "source": [
    "#### Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b4024775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    This function calculates the loss function\n",
    "    \"\"\"\n",
    "\n",
    "    # Setting a minimum and maximum value to prevent log(0) and log(1)\n",
    "    # epsilon = 1e-7\n",
    "    # y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "\n",
    "    # Binary Negative Log-Likelihood \n",
    "    epsilon = 1e-7\n",
    "    return -tf.reduce_mean(y_true * tf.math.log(y_pred + epsilon) + (1 - y_true) * tf.math.log(1 - y_pred + epsilon))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753a2eca",
   "metadata": {},
   "source": [
    "#### Define function to perform prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bdbf2168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(output):\n",
    "    \"\"\" \n",
    "    This function calculates the sigmoid function.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + tf.exp(-output))\n",
    "\n",
    "def relu(inputs):\n",
    "    \"\"\" \n",
    "    This function calculates the ReLU function.\n",
    "    \"\"\"\n",
    "    return tf.maximum(0, inputs)\n",
    "\n",
    "def forward(inputs, weights, biases):\n",
    "    \"\"\" \n",
    "    This function calculates the forward pass (predicts the label).\n",
    "    \"\"\"\n",
    "    activation = 0\n",
    "    num_layers = len(weights) - 1\n",
    "    for i in range(num_layers):\n",
    "        weighted_sum = tf.matmul(inputs, weights[i], False, False) + biases[i]\n",
    "        activation = relu(weighted_sum)\n",
    "\n",
    "    weighted_sum_output = tf.matmul(activation, weights[-1], False, False) + biases[-1]\n",
    "    prediction = sigmoid(weighted_sum_output)\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bc735b",
   "metadata": {},
   "source": [
    "#### Define function for model training\n",
    "Display the training and validation loss values for each epoch of the training loop. The displayed value must be in 6 decimal places.<br>\n",
    "Hint: <br>\n",
    "Use `tf.GradientTape` to compute the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fe17ccc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(inputs, targets, weights, biases, learning_rate):\n",
    "    \"\"\" \n",
    "    This function performs the forward pass, computes the gradient and update the weights and biases.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        outputs = forward(inputs, weights, biases)\n",
    "        loss = loss_fn(targets, outputs)\n",
    "\n",
    "    for i in range(len(weights)):\n",
    "        weights_grad = tape.gradient(loss, weights[i])\n",
    "        biases_grad = tape.gradient(loss, biases[i])\n",
    "        # Update weights and biases\n",
    "        weights[i].assign_sub(learning_rate * weights_grad)\n",
    "        biases[i].assign_sub(learning_rate * biases_grad)\n",
    "\n",
    "    # Delete the tape after using it\n",
    "    del tape\n",
    "\n",
    "    return weights, biases, loss\n",
    "\n",
    "def fit(train_dataset, valid_dataset, weights, biases, learning_rate, epochs):\n",
    "    \"\"\" \n",
    "    This function implements the training loop.\n",
    "    \"\"\"\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training loop\n",
    "        for inputs_batch, targets_batch in train_dataset:\n",
    "            weights, biases, loss = train(inputs_batch, targets_batch, weights, biases, learning_rate)\n",
    "        train_losses.append(loss)\n",
    "\n",
    "        # Validation loop\n",
    "        valid_loss_avg = 0\n",
    "        valid_steps = 0\n",
    "        for valid_inputs, valid_targets in valid_dataset:\n",
    "            valid_outputs = forward(valid_inputs, weights, biases)\n",
    "            valid_loss = loss_fn(valid_targets, valid_outputs)\n",
    "            valid_loss_avg += valid_loss\n",
    "            valid_steps += 1\n",
    "        avg_valid_loss = valid_loss_avg / valid_steps\n",
    "        valid_losses.append(avg_valid_loss)\n",
    "\n",
    "        print(f\"Epoch: {epoch}, Train Loss: {loss}, Validation Loss: {avg_valid_loss}\")\n",
    "\n",
    "    return weights, biases, train_losses, valid_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f45213",
   "metadata": {},
   "source": [
    "#### Define the tensors to hold the weights and biases (create the model)\n",
    "Hint: <br>\n",
    "Use `tf.Variable` to create the tensors.<br>\n",
    "Put the tensors in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a2e2172b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = dataset.iloc[:, :-1]  # All rows, all columns except the last\n",
    "labels = dataset.iloc[:, -1]  # All rows, only the last column\n",
    "\n",
    "# Convert the pandas DataFrame into a TensorFlow Dataset\n",
    "dataset_tf = tf.data.Dataset.from_tensor_slices((features.values, labels.values))\n",
    "\n",
    "input_size = 5  # 5 features\n",
    "hidden_size = 8  # Size of the hidden layer\n",
    "output_size = 1  # Predict 2 classes\n",
    "\n",
    "# Initialize weights and biases for each layer in lists\n",
    "weights = [tf.Variable(tf.random.normal([input_size, hidden_size], stddev=0.1)),\n",
    "           tf.Variable(tf.random.normal([hidden_size, output_size], stddev=0.1))]\n",
    "\n",
    "biases = [tf.Variable(tf.zeros([hidden_size])),\n",
    "          tf.Variable(tf.zeros([output_size]))]\n",
    "\n",
    "learning_rate = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176badb8",
   "metadata": {},
   "source": [
    "#### Split the dataset\n",
    "The ratio of training and test is 7:1:2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5fa1b9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset\n",
    "dataset_tf = dataset_tf.shuffle(buffer_size=len(dataset_tf), seed=42)\n",
    "\n",
    "# Calculate the number of examples\n",
    "total_size = len(dataset_tf)\n",
    "train_size = int(0.7 * total_size)\n",
    "valid_size = int(0.1 * total_size)\n",
    "# The rest is used for testing\n",
    "\n",
    "# Create the training, validation and test sets\n",
    "train_dataset = dataset_tf.take(train_size)\n",
    "valid_dataset = dataset_tf.skip(train_size).take(valid_size)\n",
    "test_dataset = dataset_tf.skip(train_size + valid_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c4d6cf",
   "metadata": {},
   "source": [
    "#### Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f689b7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to normalize the data\n",
    "def normalize_data(features, labels):\n",
    "    features = tf.cast(features, tf.float32)\n",
    "    features = (features - tf.reduce_min(features)) / (tf.reduce_max(features) - tf.reduce_min(features))\n",
    "    return features, labels\n",
    "\n",
    "# Apply the normalization function to the datasets\n",
    "train_dataset = train_dataset.map(normalize_data)\n",
    "valid_dataset = valid_dataset.map(normalize_data)\n",
    "test_dataset = test_dataset.map(normalize_data)\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.batch(batch_size=32, drop_remainder=True)\n",
    "valid_dataset = valid_dataset.batch(batch_size=32, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a2e7d6",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6304c496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 0.694969117641449, Validation Loss: 0.6939039826393127\n",
      "Epoch: 1, Train Loss: 0.6939254403114319, Validation Loss: 0.6951074600219727\n",
      "Epoch: 2, Train Loss: 0.6937335729598999, Validation Loss: 0.6932508945465088\n",
      "Epoch: 3, Train Loss: 0.6934312582015991, Validation Loss: 0.6935362815856934\n",
      "Epoch: 4, Train Loss: 0.6957816481590271, Validation Loss: 0.6928173899650574\n",
      "Epoch: 5, Train Loss: 0.6924941539764404, Validation Loss: 0.6926401257514954\n",
      "Epoch: 6, Train Loss: 0.6928074955940247, Validation Loss: 0.6935842633247375\n",
      "Epoch: 7, Train Loss: 0.7011305093765259, Validation Loss: 0.6924273371696472\n",
      "Epoch: 8, Train Loss: 0.6931825876235962, Validation Loss: 0.692958652973175\n",
      "Epoch: 9, Train Loss: 0.6932671070098877, Validation Loss: 0.6930678486824036\n",
      "Epoch: 10, Train Loss: 0.6945281028747559, Validation Loss: 0.6933467984199524\n",
      "Epoch: 11, Train Loss: 0.6926710605621338, Validation Loss: 0.6928125023841858\n",
      "Epoch: 12, Train Loss: 0.6932002305984497, Validation Loss: 0.6924042701721191\n",
      "Epoch: 13, Train Loss: 0.6927818655967712, Validation Loss: 0.6935110092163086\n",
      "Epoch: 14, Train Loss: 0.6932175159454346, Validation Loss: 0.6928990483283997\n",
      "Epoch: 15, Train Loss: 0.6948655843734741, Validation Loss: 0.6929846405982971\n",
      "Epoch: 16, Train Loss: 0.6931972503662109, Validation Loss: 0.6929166913032532\n",
      "Epoch: 17, Train Loss: 0.69272780418396, Validation Loss: 0.6946887969970703\n",
      "Epoch: 18, Train Loss: 0.6926891803741455, Validation Loss: 0.6923448443412781\n",
      "Epoch: 19, Train Loss: 0.6930570602416992, Validation Loss: 0.6931524872779846\n",
      "Epoch: 20, Train Loss: 0.6922042369842529, Validation Loss: 0.6926023960113525\n",
      "Epoch: 21, Train Loss: 0.6932807564735413, Validation Loss: 0.6933169960975647\n",
      "Epoch: 22, Train Loss: 0.6932069063186646, Validation Loss: 0.692980945110321\n",
      "Epoch: 23, Train Loss: 0.6921823024749756, Validation Loss: 0.6932803988456726\n",
      "Epoch: 24, Train Loss: 0.6926877498626709, Validation Loss: 0.6920104026794434\n",
      "Epoch: 25, Train Loss: 0.6931387782096863, Validation Loss: 0.6931123733520508\n",
      "Epoch: 26, Train Loss: 0.6919716596603394, Validation Loss: 0.693098783493042\n",
      "Epoch: 27, Train Loss: 0.6932219862937927, Validation Loss: 0.693082332611084\n",
      "Epoch: 28, Train Loss: 0.6930609941482544, Validation Loss: 0.6920803189277649\n",
      "Epoch: 29, Train Loss: 0.6939825415611267, Validation Loss: 0.6911517977714539\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "weights, biases, train_losses, valid_losses = fit(train_dataset, valid_dataset, weights, biases, learning_rate, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c41885",
   "metadata": {},
   "source": [
    "#### Display the training loss and validation loss against epoch graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "47f05472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0b4c38",
   "metadata": {},
   "source": [
    "#### Predict the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84f73b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20d715ef",
   "metadata": {},
   "source": [
    "#### Display the confusion matrix and the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35deeb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
